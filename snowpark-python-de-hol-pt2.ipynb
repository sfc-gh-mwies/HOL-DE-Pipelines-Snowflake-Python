{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ece550-434d-459b-9a6a-c913037dd791",
   "metadata": {
    "name": "cell45",
    "collapsed": false
   },
   "source": "# Data Engineering Pipelines with Snowpark Python - Part 2!\n\n\nWelcomg to Part 2 of the Hands-on Lab! If you have not completed Part 1, please do so as it is required to set up the database objects for this part of the lab. \n\nAs a reminder, run the next cell to see a visual overview of what we're building:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfc92d-977c-4019-a64e-a297b49b356a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell46"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.image(\"https://raw.githubusercontent.com/Snowflake-Labs/sfguide-data-engineering-with-snowpark-python/main/images/demo_overview.png\",width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c2c63-c904-46aa-b0e4-dddaa502d5a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell47"
   },
   "source": [
    "# Orchestrate Jobs\n",
    "\n",
    "During this step we will be orchestrating our new Snowpark pipelines with Snowflake's native orchestration feature named Tasks. We will create two tasks, one for each stored procedure, and chain them together. We will then run the tasks. To put this in context, we are on step **#8** in our data flow overview.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ba473-5fb5-4ecd-a99c-99c1dcc10f0e",
   "metadata": {
    "collapsed": false,
    "name": "cell48"
   },
   "source": "### Creating and Running the Tasks\nIn this step we did not create a schedule for our task DAG, so it will not run on its own at this point. So in this script you will notice that we manually execute the DAG, using the `EXECUTE TASK` command."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868473c7-aea4-430a-be35-7f710fc34729",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "cell49"
   },
   "outputs": [],
   "source": "-- ----------------------------------------------------------------------------\n-- Step #1: Create the tasks to call our Python stored procedures\n-- ----------------------------------------------------------------------------\n\nUSE NB_HOL_DB.HARMONIZED;\nUSE WAREHOUSE HOL_WH;\n\nCREATE OR REPLACE TASK ORDERS_UPDATE_TASK\nWAREHOUSE = HOL_WH\nWHEN\n  SYSTEM$STREAM_HAS_DATA('POS_FLATTENED_V_STREAM')\nAS\nCALL HARMONIZED.ORDERS_UPDATE_SP();\n\nCREATE OR REPLACE TASK DAILY_CITY_METRICS_UPDATE_TASK\nWAREHOUSE = HOL_WH\nAFTER ORDERS_UPDATE_TASK\nWHEN\n  SYSTEM$STREAM_HAS_DATA('ORDERS_STREAM')\nAS\nCALL HARMONIZED.DAILY_CITY_METRICS_UPDATE_SP();\n\n-- ----------------------------------------------------------------------------\n-- Step #2: Execute the tasks\n-- ----------------------------------------------------------------------------\n\nALTER TASK DAILY_CITY_METRICS_UPDATE_TASK RESUME;\n\nEXECUTE TASK ORDERS_UPDATE_TASK;"
  },
  {
   "cell_type": "markdown",
   "id": "66c037b3-bf7e-45a8-b59d-dc38151a2e1f",
   "metadata": {
    "name": "cell50",
    "collapsed": false
   },
   "source": "\nTo see what happened when you ran this task just now, run the following query:\n"
  },
  {
   "cell_type": "code",
   "id": "a6adef99-2f60-4c89-99b2-adc2e12e0a07",
   "metadata": {
    "language": "sql",
    "name": "cell51",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Task execution history in the past day\nSELECT SCHEDULED_TIME,NAME,STATE,DATABASE_NAME,SCHEMA_NAME,QUERY_TEXT,CONDITION_TEXT,QUERY_START_TIME,COMPLETED_TIME,QUERY_ID\nFROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n    SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n    SCHEDULED_TIME_RANGE_END=>DATEADD('MINUTE',1,CURRENT_TIMESTAMP()),\n    RESULT_LIMIT => 100))\nORDER BY SCHEDULED_TIME DESC\n;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64c61812-3b50-4e79-8d5b-7c762d92c681",
   "metadata": {
    "name": "cell52",
    "collapsed": false
   },
   "source": "\nYou will notice in the task history output that it skipped our task `ORDERS_UPDATE_TASK`. This is correct, because our `HARMONIZED.POS_FLATTENED_V_STREAM` stream doesn't have any data. We'll add some new data and run them again in the next step.\n\n### More on Tasks\nTasks are Snowflake's native scheduling/orchestration feature. With a task you can execute any one of the following types of SQL code:\n\n* Single SQL statement\n* Call to a stored procedure\n* Procedural logic using Snowflake Scripting Developer Guide\n\nFor this Quickstart we'll `CALL` our Snowpark stored procedures. \n\nA few things to point out; First you specify which Snowflake virtual warehouse to use when running the task with the `WAREHOUSE` clause. The `AFTER` clause lets you define the relationship between tasks, and the structure of this relationship is a Directed Acyclic Graph (or DAG) like most orchestration tools provide. The `AS` clause let's you define what the task should do when it runs, in this case to call our stored procedure.\n\nThe `WHEN` clause is really cool. We've already seen how streams work in Snowflake by allowing you to incrementally process data. We've even seen how you can create a stream on a view (which joins many tables together) and create a stream on that view to process its data incrementally! Here in the `WHEN` clause we're calling a system function `SYSTEM$STREAM_HAS_DATA()` which returns true if the specified stream has new data. With the `WHEN` clause in place the virtual warehouse will only be started up when the stream has new data. So if there's no new data when the task runs then your warehouse won't be started up and you won't be charged. You will only be charged when there's new data to process. Pretty cool, huh?\n\nAs mentioned above we did not define a `SCHEDULE` for the root task, so this DAG will not run on its own. That's fine for this Quickstart, but in a real situation you would define a schedule. See [CREATE TASK](https://docs.snowflake.com/en/sql-reference/sql/create-task.html) for the details.\n\nAnd for more details on Tasks see [Introduction to Tasks](https://docs.snowflake.com/en/user-guide/tasks-intro.html).\n\n### Task Metadata\nSnowflake keeps metadata for almost everything you do, and makes that metadata available for you to query (and to create any type of process around). Tasks are no different, Snowflake maintains rich metadata to help you monitor your task runs. Here are a few sample SQL queries you can use to monitor your tasks runs:"
  },
  {
   "cell_type": "code",
   "id": "8358a950-38cd-445b-8c7e-7c9b9b845857",
   "metadata": {
    "language": "sql",
    "name": "cell53",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Get a list of tasks\nSHOW TASKS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27128236-78a4-4def-b5a8-137e887301be",
   "metadata": {
    "name": "cell54",
    "collapsed": false
   },
   "source": "### Monitoring Tasks\nSo while you're free to create any operational or monitoring process you wish, Snowflake provides some rich task observability features in our Snowsight UI. Try it out for yourself by following these steps (in a new browser tab):\n\n1. In the Snowsight navigation menu, click **Data** » **Databases**.\n1. In the right pane, using the object explorer, navigate to a database and schema.\n1. For the selected schema, select and expand **Tasks**.\n1. Select a task. Task information is displayed, including **Task Details**, **Graph**, and **Run History** sub-tabs.\n1. Select the **Graph** tab. The task graph appears, displaying a hierarchy of child tasks.\n1. Select a task to view its details."
  },
  {
   "cell_type": "markdown",
   "id": "0439490a-61bd-45cd-b99b-357de4c86efa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell55"
   },
   "source": "\n# Process Incrementally\n\nDuring this step we will be adding new data to our POS order tables and then running our entire end-to-end pipeline to process the new data. \n\nThis entire pipeline will be processing data incrementally thanks to Snowflake's advanced stream/CDC capabilities. To put this in context, we are on step **#9** in our data flow overview.\n\nFirst, we'll dynamically scale up our warehouse to `XLARGE` and **LS** the directory to preview the files that will be loaded"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8084fdd-ad00-4d02-b762-bbd076d17bd3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell56"
   },
   "outputs": [],
   "source": "-- ----------------------------------------------------------------------------\n-- Add new/remaining order data\n-- ----------------------------------------------------------------------------\n\nUSE SCHEMA NB_HOL_DB.RAW_POS;\n\nALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE;\n\nLS @external.frostbyte_raw_stage/pos/order_header/year=2022"
  },
  {
   "cell_type": "markdown",
   "id": "5c9a7ab2-fa80-480a-889e-9d1fb1326387",
   "metadata": {
    "name": "cell57",
    "collapsed": false
   },
   "source": "Everything look good, so let's go aehad and run the `COPY INTO` and load the ORDER_DETAIL records from 2022."
  },
  {
   "cell_type": "code",
   "id": "cf3a2031-5b54-49dc-8f2c-fc368f844bda",
   "metadata": {
    "language": "sql",
    "name": "cell58",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "COPY INTO ORDER_HEADER FROM @external.frostbyte_raw_stage/pos/order_header/year=2022\nFILE_FORMAT = (FORMAT_NAME = EXTERNAL.PARQUET_FORMAT)\nMATCH_BY_COLUMN_NAME = CASE_SENSITIVE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac79c7-a351-41e2-b80a-96c2d899c363",
   "metadata": {
    "language": "sql",
    "name": "cell59",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "COPY INTO ORDER_DETAIL FROM @external.frostbyte_raw_stage/pos/order_detail/year=2022\n",
    "FILE_FORMAT = (FORMAT_NAME = EXTERNAL.PARQUET_FORMAT)\n",
    "MATCH_BY_COLUMN_NAME = CASE_SENSITIVE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c115b7d4-b57a-4ed4-8025-bd5571ee57a0",
   "metadata": {
    "name": "cell60",
    "collapsed": false
   },
   "source": "Before we kick off our pipeline, take a look at the stream we created on top of the POS data. It should now contain all the new records we just copied into the ORDER_DETAIL table."
  },
  {
   "cell_type": "code",
   "id": "bab2fa81-c5a7-48ed-8671-c49452932afa",
   "metadata": {
    "language": "sql",
    "name": "cell61",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM NB_HOL_DB.HARMONIZED.POS_FLATTENED_V_STREAM limit 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e0bfdc2-4026-45ae-9f55-a89727d4383e",
   "metadata": {
    "language": "sql",
    "name": "cell113",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT COUNT(1) FROM NB_HOL_DB.HARMONIZED.POS_FLATTENED_V_STREAM AS NEW_ROWS_TO_PROCESS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "682d8a14-fe3d-4fe2-a6d5-02f58183e396",
   "metadata": {
    "name": "cell114",
    "collapsed": false
   },
   "source": "☝️ Remember: These queries don't affect the stream buffer (i.e. the high water mark) because they are simply SELECT statements. \n\nOnly DML statements which insert the records from the stream into a downstream table will automatically clear the stream.\n\nAlright, now let's kick off our pipeline:"
  },
  {
   "cell_type": "code",
   "id": "1e71f87f-9d0d-4d7a-b11a-d11c7c47632d",
   "metadata": {
    "language": "sql",
    "name": "cell115",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- First, let's grab a count of the # records in our orders table and city metrics table before the udpate\nSELECT * FROM\n    (SELECT COUNT(1) AS ORDER_COUNT_BEFORE FROM NB_HOL_DB.HARMONIZED.ORDERS)a,\n    (SELECT COUNT(1) AS DAILY_METRICS_COUNT_BEFORE FROM NB_HOL_DB.ANALYTICS.DAILY_CITY_METRICS)b\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b168c-c5c0-4af7-86f4-7ade69949283",
   "metadata": {
    "language": "sql",
    "name": "cell116",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Kick off the pipeline\nEXECUTE TASK NB_HOL_DB.HARMONIZED.ORDERS_UPDATE_TASK;\n\nALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL;"
  },
  {
   "cell_type": "markdown",
   "id": "5009d825-7703-4370-a26a-072aaf7a8a56",
   "metadata": {
    "name": "cell117a",
    "collapsed": false
   },
   "source": "### Viewing the Task History\nLike the in the previous step, to see what happened when you ran this task DAG, run this query:"
  },
  {
   "cell_type": "code",
   "id": "778b21ad-a75d-4bd7-8e16-067c91f0dc6e",
   "metadata": {
    "language": "sql",
    "name": "cell117b",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT SCHEDULED_TIME,NAME,STATE,DATABASE_NAME,SCHEMA_NAME,QUERY_TEXT,CONDITION_TEXT,QUERY_START_TIME,COMPLETED_TIME,QUERY_ID\nFROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n    SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n    SCHEDULED_TIME_RANGE_END=>DATEADD('MINUTE',1,CURRENT_TIMESTAMP()),\n    RESULT_LIMIT => 100))\nORDER BY SCHEDULED_TIME DESC\n;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "87fabf4e-c723-4cce-b188-947e1612a21a",
   "metadata": {
    "name": "cell118",
    "collapsed": false
   },
   "source": "This time you will notice that the `ORDERS_UPDATE_TASK` task will not be skipped, since the `HARMONIZED.POS_FLATTENED_V_STREAM` stream has new data. In a few minutes you should see that both the `ORDERS_UPDATE_TASK` task and the `DAILY_CITY_METRICS_UPDATE_TASK` task completed successfully.\n\n### Query History for Tasks\nOne important thing to understand about tasks, is that the queries which get executed by the task won't show up with the default Query History UI settings. In order to see the queries that just ran you need to do the following:\n* Remove filters at the top of this table, including your username, as later scheduled tasks will run as \"System\"\n* Click \"Filter\", and add filter option 'Queries executed by user tasks' and click \"Apply Filters\"\n\nYou should now see all the queries run by your tasks! Take a look at each of the MERGE commands in the Query History to see how many records were processed by each task. And don't forget to notice that we processed the whole pipeline just now, and did so incrementally!"
  },
  {
   "cell_type": "markdown",
   "id": "ef47cbae-164e-48b2-8ab0-fafb6b4b09ab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell119"
   },
   "source": [
    "## Teardown\n",
    "Once you're finished with the Quickstart and want to clean things up, you can simply run the following commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5625e-83a3-405b-a78b-07d82088ef7e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell120"
   },
   "outputs": [],
   "source": "DROP DATABASE NB_HOL_DB;\nDROP WAREHOUSE HOL_WH;"
  },
  {
   "cell_type": "markdown",
   "id": "bb371043-ba33-4d70-bc49-e388162c6e82",
   "metadata": {
    "name": "cell121",
    "collapsed": false
   },
   "source": [
    "### What we've covered\n",
    "We've covered a ton in this Quickstart, and here are the highlights:\n",
    "\n",
    "* Snowflake's Table Format\n",
    "* Data ingestion with COPY\n",
    "* Schema inference\n",
    "* Data sharing/marketplace (instead of ETL)\n",
    "* Streams for incremental processing (CDC)\n",
    "* Streams on views\n",
    "* Python UDFs (with third-party packages)\n",
    "* Python Stored Procedures\n",
    "* Snowpark DataFrame API\n",
    "* Snowpark Python programmability\n",
    "* Warehouse elasticity (dynamic scaling)\n",
    "* Visual Studio Code Snowflake native extension (PuPr, Git integration)\n",
    "* SnowCLI (PuPr)\n",
    "* Tasks (with Stream triggers)\n",
    "* Task Observability\n",
    "* GitHub Actions (CI/CD) integration\n",
    "\n",
    "### Related Resources\n",
    "And finally, here's a quick recap of related resources:\n",
    "\n",
    "* [Full Demo on Snowflake Demo Hub](https://developers.snowflake.com/demos/data-engineering-pipelines/)\n",
    "* [Source Code on GitHub](https://github.com/Snowflake-Labs/sfguide-data-engineering-with-snowpark-python)\n",
    "* [Snowpark Developer Guide for Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/index.html)\n",
    "    * [Writing Python UDFs](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python.html)\n",
    "    * [Writing Stored Procedures in Snowpark (Python)](https://docs.snowflake.com/en/sql-reference/stored-procedures-python.html)\n",
    "    * [Working with DataFrames in Snowpark Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes.html)\n",
    "* Related Tools\n",
    "    * [Snowflake Visual Studio Code Extension](https://marketplace.visualstudio.com/items?itemName=snowflake.snowflake-vsc)\n",
    "    * [SnowCLI Tool](https://github.com/Snowflake-Labs/snowcli)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeab7fa-9e76-4605-88a5-3a531865bc1a",
   "metadata": {
    "name": "cell122",
    "collapsed": false
   },
   "source": ""
  }
 ]
}