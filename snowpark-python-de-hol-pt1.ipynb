{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ece550-434d-459b-9a6a-c913037dd791",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# Data Engineering Pipelines with Snowpark Python\n\n\nAre you interested in unleashing the power of Snowpark Python to build data engineering pipelines? Well then, this Hands-on Lab is for you! The focus here will be on building data engineering pipelines with Python, and not on data science. For examples of doing data science with Snowpark Python please check out our [Machine Learning with Snowpark Python: - Credit Card Approval Prediction](https://quickstarts.snowflake.com/guide/getting_started_snowpark_machine_learning/index.html?index=..%2F..index#0) Quickstart.\n\nThis Lab is a modified version of [This Quickstart](https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_python/index.html) and captures all the content of the quickstart in this convenient Snowflake Notebook directly in your Snowflake account. \n\nRun the next cell to see a visual overview of what we're going to build:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfc92d-977c-4019-a64e-a297b49b356a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "import streamlit as st\nst.image(\"https://raw.githubusercontent.com/Snowflake-Labs/sfguide-data-engineering-with-snowpark-python/main/images/demo_overview.png\",width=800)\n"
  },
  {
   "cell_type": "markdown",
   "id": "5a796a19-195f-488b-8a54-240de881582c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell3a"
   },
   "source": "### Prerequisites\n* This lab assumes at least introductory experience with Python and Snowflake**A Snowflake Account**\n* **A Snowflake user created with ACCOUNTADMIN permissions**. This user will be used to get things setup in Snowflake.\n* **Anaconda Terms & Conditions accepted**. See Getting Started section in [Third-Party Packages](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-packages.html#getting-started).\n\n### What You’ll Learn\nYou will learn about the following Snowflake features during this Quickstart:\n\n* Snowflake's Table Format\n* Data ingestion with COPY\n* Schema inference\n* Data sharing/marketplace (instead of ETL)\n* Streams for incremental processing (CDC)\n* Streams on views\n* Python UDFs (with third-party packages)\n* Python Stored Procedures\n* Snowpark DataFrame API\n* Snowpark Python programmability\n* Warehouse elasticity (dynamic scaling)\n* SnowCLI (PuPr)\n* Tasks (with Stream triggers)\n* Task Observability\n\n### What You’ll Build\nDuring this Quickstart you will accomplish the following things:\n\n* Load Parquet data to Snowflake using schema inference\n* Setup access to Snowflake Marketplace data\n* Create a Python UDF to convert temperature\n* Create a data engineering pipeline with Python stored procedures to incrementally process data\n* Orchestrate the pipelines with tasks\n* Monitor the pipelines with Snowsight\n* Deploy the Snowpark Python stored procedures via a CI/CD pipeline\n\n### Infographic explaining the [Tasty Bytes Food Truck Business](https://quickstarts.snowflake.com/guide/tasty_bytes_introduction/img/a51f501137dea3c5.png)"
  },
  {
   "cell_type": "markdown",
   "id": "7fd451b3-daca-4a6c-ad61-57c7339cda00",
   "metadata": {
    "name": "cell3b",
    "collapsed": false
   },
   "source": "\n## Setup Script\n**Important:** *Read through the next two cells to understand the objects involved in the Lab*\n\nOnce you've read the code, run it by clicking the 'play' button in the top right corner of each cell, or press cmd/ctrl+Enter on your keyboard while the cell is active"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551d60d-f4e1-44cb-be72-ce26d4814c50",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- ----------------------------------------------------------------------------\n-- Create the account level objects\n-- ----------------------------------------------------------------------------\nUSE ROLE ACCOUNTADMIN;\n\n-- Roles\nSET MY_USER = CURRENT_USER(); \nCREATE OR REPLACE ROLE HOL_ROLE;\nGRANT ROLE HOL_ROLE TO ROLE SYSADMIN;\nGRANT ROLE HOL_ROLE TO USER IDENTIFIER($MY_USER);\n\nCREATE STAGE IF NOT EXISTS SCRIPTS \n\tDIRECTORY = ( ENABLE = true );\n\nGRANT EXECUTE TASK ON ACCOUNT TO ROLE HOL_ROLE;\nGRANT MONITOR EXECUTION ON ACCOUNT TO ROLE HOL_ROLE;\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE HOL_ROLE;\n\n-- Databases\nCREATE OR REPLACE DATABASE NB_HOL_DB;\nGRANT OWNERSHIP ON DATABASE NB_HOL_DB TO ROLE HOL_ROLE;\n\n-- Warehouses\nCREATE OR REPLACE WAREHOUSE HOL_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\nGRANT OWNERSHIP ON WAREHOUSE HOL_WH TO ROLE HOL_ROLE;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706ef72-370b-41c4-a46f-bb3eb8f58f45",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- ----------------------------------------------------------------------------\n-- Create the database level objects\n-- ----------------------------------------------------------------------------\nUSE ROLE HOL_ROLE;\nUSE WAREHOUSE HOL_WH;\nUSE DATABASE NB_HOL_DB;\n\n-- Schemas\nCREATE OR REPLACE SCHEMA EXTERNAL;\nCREATE OR REPLACE SCHEMA RAW_POS;\nCREATE OR REPLACE SCHEMA RAW_CUSTOMER;\nCREATE OR REPLACE SCHEMA HARMONIZED;\nCREATE OR REPLACE SCHEMA ANALYTICS;\n\n-- External Frostbyte objects\nUSE SCHEMA EXTERNAL;\nCREATE OR REPLACE FILE FORMAT PARQUET_FORMAT\n    TYPE = PARQUET\n    COMPRESSION = SNAPPY;\n    \nCREATE OR REPLACE STAGE FROSTBYTE_RAW_STAGE\n    URL = 's3://sfquickstarts/data-engineering-with-snowpark-python/';\n\n-- ANALYTICS objects\nUSE SCHEMA ANALYTICS;\n\nCREATE OR REPLACE FUNCTION ANALYTICS.INCH_TO_MILLIMETER_UDF(INCH NUMBER(35,4))\nRETURNS NUMBER(35,4)\n    AS\n$$\n    inch * 25.4\n$$;"
  },
  {
   "cell_type": "markdown",
   "id": "e2993fd0-56ed-4f8f-bf14-2d61fc433ae1",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "### Load Raw \n\nDuring this step we will be loading the raw Tasty Bytes POS and Customer loyalty data from raw Parquet files in `s3://sfquickstarts/data-engineering-with-snowpark-python/` to our `RAW_POS` and `RAW_CUSTOMER` schemas in Snowflake. To put this in context, we are on step **#2** in our data flow overview:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a0d00-69d2-4aae-9c68-755d5c30fb4c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.image(\"https://raw.githubusercontent.com/Snowflake-Labs/sfguide-data-engineering-with-snowpark-python/main/images/demo_overview.png\",width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272987f-0f1e-44e2-b2dc-4359906ddf4e",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "The following cell contains three functions; two to load raw data, and one to validate the raw tables."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbe721-66fc-4031-b3fe-6f195ee5baa7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "import time\nfrom snowflake.snowpark import Session\n#import snowflake.snowpark.types as T\n#import snowflake.snowpark.functions as F\n\n\nPOS_TABLES = ['country', 'franchise', 'location', 'menu', 'truck', 'order_header', 'order_detail']\nCUSTOMER_TABLES = ['customer_loyalty']\nTABLE_DICT = {\n    \"pos\": {\"schema\": \"RAW_POS\", \"tables\": POS_TABLES},\n    \"customer\": {\"schema\": \"RAW_CUSTOMER\", \"tables\": CUSTOMER_TABLES}\n}\n\n# SNOWFLAKE ADVANTAGE: Schema detection\n# SNOWFLAKE ADVANTAGE: Data ingestion with COPY\n# SNOWFLAKE ADVANTAGE: Snowflake Tables (not file-based)\n\ndef load_raw_table(session, tname=None, s3dir=None, year=None, schema=None):\n    session.use_schema(schema)\n    if year is None:\n        location = \"@external.frostbyte_raw_stage/{}/{}\".format(s3dir, tname)\n    else:\n        print('\\tLoading year {}'.format(year)) \n        location = \"@external.frostbyte_raw_stage/{}/{}/year={}\".format(s3dir, tname, year)\n    \n    # we can infer schema using the parquet read option\n    df = session.read.option(\"compression\", \"snappy\") \\\n                            .parquet(location)\n    df.copy_into_table(\"{}\".format(tname))\n\n# SNOWFLAKE ADVANTAGE: Warehouse elasticity (dynamic scaling)\n\ndef load_all_raw_tables(session):\n    _ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n\n    for s3dir, data in TABLE_DICT.items():\n        tnames = data['tables']\n        schema = data['schema']\n        for tname in tnames:\n            print(\"Loading {}\".format(tname))\n            # Only load the first 3 years of data for the order tables at this point\n            # We will load the 2022 data later in the lab\n            if tname in ['order_header', 'order_detail']:\n                # For testing, use only records from 2019. (Un)comment to include years.\n                for year in ['2019']:#, '2020', '2021']: \n                    load_raw_table(session, tname=tname, s3dir=s3dir, year=year, schema=schema)\n            else:\n                load_raw_table(session, tname=tname, s3dir=s3dir, schema=schema)\n\n    _ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n\ndef validate_raw_tables(session):\n    # check column names from the inferred schema\n    for tname in POS_TABLES:\n        print('{}: \\n\\t{}\\n'.format(tname, session.table('RAW_POS.{}'.format(tname)).columns))\n\n    for tname in CUSTOMER_TABLES:\n        print('{}: \\n\\t{}\\n'.format(tname, session.table('RAW_CUSTOMER.{}'.format(tname)).columns))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf12e2-9257-47b0-8c55-c7d87cce6267",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624c8ad-f94b-4cf2-b06d-76b32b39a507",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "# Call the function created above to load all raw tables\n# Monitor the output and wait until the cell is finished running before continuing\nload_all_raw_tables(session)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62214d74-b02d-4868-8ad2-e6967d581ab0",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "# Once raw data is loaded, validate the customer and POS table with the function created previously\nvalidate_raw_tables(session)"
  },
  {
   "cell_type": "markdown",
   "id": "9c058d54-6ad1-429b-813b-d8309a433f7a",
   "metadata": {
    "collapsed": false,
    "name": "cell14"
   },
   "source": "### Viewing What Happened in Snowflake\n---\n#### `ACTION:` Duplicate this browser tab (or open a new window and navigate to your account URL) and take a quick look at the query history in the left pane. You can see the SQL that was generated by the Snowpark API. This will help you better understand what the API is doing and will help you debug any issues you may run into.\n---\nThe [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity.html#query-history) in Snowflake is a powerful feature. It logs every query run against your Snowflake account, no matter which tool or process initiated it. And this is especially helpful when working with client tools and APIs.\n\nThe Python script you just ran did a small amount of work locally, basically just orchestrating the process by looping through each table and issuing the command to Snowflake to load the data. But all of the heavy lifting ran inside Snowflake! This push-down is a hallmark of the Snowpark API and allows you to leverage the scalability and compute power of Snowflake.\n\n\n### Schema Inference\nOne very helpful feature in Snowflake is the ability to infer the schema of files in a stage that you wish to work with. This is accomplished in SQL with the [`INFER_SCHEMA()`](https://docs.snowflake.com/en/sql-reference/functions/infer_schema.html) function. The Snowpark Python API does this for you automatically when you call the `session.read()` method. **Can** you find the session.read() method in the code we just ran?\n\n### Data Ingestion with COPY\nIn order to load the data into a Snowflake table we used the `copy_into_table()` method on a DataFrame. This method creates the target table in Snowflake using the inferred schema (if it doesn't exist), and then calls the highly optimized Snowflake `COPY INTO TABLE;` [Command](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html).\n\n### Snowflake's Table Format\nOne of the major advantages of Snowflake is being able to eliminate the need to manage a file-based data lake. And Snowflake was designed for this purpose from the beginning. In the next step we are loading the raw data into a structured Snowflake managed table. But Snowflake tables can natively support structured and semi-structured data, and are stored in Snowflake's mature cloud table format.\n\n### Warehouse Elasticity (Dynamic Scaling)\nWith Snowflake there is only one type of user-defined compute cluster, the [Virtual Warehouse](https://docs.snowflake.com/en/user-guide/warehouses.html), regardless of the language you use to process that data (SQL, Python, Java, Scala, Javascript, etc.). This makes working with data much simpler in Snowflake.\n\nThese virtual warehouses can be instantly resized -- directly within your code -- to increase the capacity and run a section of code in a fraction of the time, then dynamically resized again to reduce the amount of capacity.\n\n**Here is this code snippet as an example of how this works:**\n\n```python\n_ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n\n# Some data processing code\n\n_ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n```\n\nNote the `WAIT_FOR_COMPLETION` parameter in the first `ALTER WAREHOUSE` statement. Setting this parameter to `TRUE` will block the return of the `ALTER WAREHOUSE` command until the resize has finished provisioning all its compute resources. This way we make sure that the full cluster is available before processing any data with it.\n\nWe will use this pattern a few more times during this Lab, so it's important to understand.\n"
  },
  {
   "cell_type": "markdown",
   "id": "64122344-5484-430d-907a-2ba3702d7172",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell15"
   },
   "source": "## Load Weather\n\n\nDuring this step we will be \"loading\" the raw weather data to Snowflake. But \"loading\" is really the wrong word here. Because we're using Snowflake's unique data sharing capability we don't actually need to copy the data to our Snowflake account. Instead we can directly access the data shared by Weather Source in the Snowflake Data Marketplace. To put this in context, we are on step **#3** in our data flow overview. \n\n\n### Snowflake Data Marketplace\nLet's connect to the `Weather Source LLC: frostbyte` feed from Weather Source in the Snowflake Data Marketplace by following these steps:\n\n* Login to Snowsight again in another browser tab\n* Click on the `Marketplace` link in the left navigation bar\n* Enter \"Weather Source LLC: frostbyte\" in the search box and click return\n* Click on the \"Weather Source LLC: frostbyte\" listing tile\n* Click the blue \"Get\" button\n    * Expand the \"Options\" dialog\n    * Change the \"Database name\" to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n    * Select the \"HOL_ROLE\" role to have access to the new database\n* Click on the blue \"Get\" button\n\nThat's it... we don't have to do anything from here to keep this data updated. The provider will do that for us and data sharing means we are always seeing whatever they have published. How amazing is that? Just think of all the things you didn't have do here to get access to an always up-to-date, third-party dataset!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8555e-4c29-4d5c-8193-d182c822baf4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": "-- Make sure you grant the table privileges to HOL_ROLE for this to run\nUSE ROLE ACCOUNTADMIN;\nGRANT IMPORTED PRIVILEGES ON DATABASE IDENTIFIER('\"FROSTBYTE_WEATHERSOURCE\"') TO ROLE IDENTIFIER('\"HOL_ROLE\"');\nUSE ROLE HOL_ROLE;\n-- Preview Weathersource data\nSELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100; "
  },
  {
   "cell_type": "markdown",
   "id": "9192a8d0-66fb-4a73-9883-531d25607a9c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell17"
   },
   "source": "## Create POS View\n\nDuring this step we will be creating a view to simplify the raw POS schema by joining together 6 different tables and picking only the columns we need. But what's really cool is that we're going to define that view with the Snowpark DataFrame API! Then we're going to create a Snowflake stream on that view so that we can incrementally process changes to any of the POS tables. To put this in context, we are on step **#4** in our data flow overview.\n\n### Run the Script\n*To create the view and stream, execute the following code:*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235cfa2-7694-4d77-bbf2-7d3c71c45ab1",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\n#import snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\ndef create_pos_view(session):\n    session.use_schema('HARMONIZED')\n    order_detail = session.table(\"RAW_POS.ORDER_DETAIL\").select(F.col(\"ORDER_DETAIL_ID\"), \\\n                                                                F.col(\"LINE_NUMBER\"), \\\n                                                                F.col(\"MENU_ITEM_ID\"), \\\n                                                                F.col(\"QUANTITY\"), \\\n                                                                F.col(\"UNIT_PRICE\"), \\\n                                                                F.col(\"PRICE\"), \\\n                                                                F.col(\"ORDER_ID\"))\n    order_header = session.table(\"RAW_POS.ORDER_HEADER\").select(F.col(\"ORDER_ID\"), \\\n                                                                F.col(\"TRUCK_ID\"), \\\n                                                                F.col(\"ORDER_TS\"), \\\n                                                                F.to_date(F.col(\"ORDER_TS\")).alias(\"ORDER_TS_DATE\"), \\\n                                                                F.col(\"ORDER_AMOUNT\"), \\\n                                                                F.col(\"ORDER_TAX_AMOUNT\"), \\\n                                                                F.col(\"ORDER_DISCOUNT_AMOUNT\"), \\\n                                                                F.col(\"LOCATION_ID\"), \\\n                                                                F.col(\"ORDER_TOTAL\"))\n    truck = session.table(\"RAW_POS.TRUCK\").select(F.col(\"TRUCK_ID\"), \\\n                                                F.col(\"PRIMARY_CITY\"), \\\n                                                F.col(\"REGION\"), \\\n                                                F.col(\"COUNTRY\"), \\\n                                                F.col(\"FRANCHISE_FLAG\"), \\\n                                                F.col(\"FRANCHISE_ID\"))\n    menu = session.table(\"RAW_POS.MENU\").select(F.col(\"MENU_ITEM_ID\"), \\\n                                                F.col(\"TRUCK_BRAND_NAME\"), \\\n                                                F.col(\"MENU_TYPE\"), \\\n                                                F.col(\"MENU_ITEM_NAME\"))\n    franchise = session.table(\"RAW_POS.FRANCHISE\").select(F.col(\"FRANCHISE_ID\"), \\\n                                                        F.col(\"FIRST_NAME\").alias(\"FRANCHISEE_FIRST_NAME\"), \\\n                                                        F.col(\"LAST_NAME\").alias(\"FRANCHISEE_LAST_NAME\"))\n    location = session.table(\"RAW_POS.LOCATION\").select(F.col(\"LOCATION_ID\"))\n\n    t_with_f = truck.join(franchise, truck['FRANCHISE_ID'] == franchise['FRANCHISE_ID'], rsuffix='_f')\n    oh_w_t_and_l = order_header.join(t_with_f, order_header['TRUCK_ID'] == t_with_f['TRUCK_ID'], rsuffix='_t') \\\n                                .join(location, order_header['LOCATION_ID'] == location['LOCATION_ID'], rsuffix='_l')\n    final_df = order_detail.join(oh_w_t_and_l, order_detail['ORDER_ID'] == oh_w_t_and_l['ORDER_ID'], rsuffix='_oh') \\\n                            .join(menu, order_detail['MENU_ITEM_ID'] == menu['MENU_ITEM_ID'], rsuffix='_m')\n    final_df = final_df.select(F.col(\"ORDER_ID\"), \\\n                            F.col(\"TRUCK_ID\"), \\\n                            F.col(\"ORDER_TS\"), \\\n                            F.col('ORDER_TS_DATE'), \\\n                            F.col(\"ORDER_DETAIL_ID\"), \\\n                            F.col(\"LINE_NUMBER\"), \\\n                            F.col(\"TRUCK_BRAND_NAME\"), \\\n                            F.col(\"MENU_TYPE\"), \\\n                            F.col(\"PRIMARY_CITY\"), \\\n                            F.col(\"REGION\"), \\\n                            F.col(\"COUNTRY\"), \\\n                            F.col(\"FRANCHISE_FLAG\"), \\\n                            F.col(\"FRANCHISE_ID\"), \\\n                            F.col(\"FRANCHISEE_FIRST_NAME\"), \\\n                            F.col(\"FRANCHISEE_LAST_NAME\"), \\\n                            F.col(\"LOCATION_ID\"), \\\n                            F.col(\"MENU_ITEM_ID\"), \\\n                            F.col(\"MENU_ITEM_NAME\"), \\\n                            F.col(\"QUANTITY\"), \\\n                            F.col(\"UNIT_PRICE\"), \\\n                            F.col(\"PRICE\"), \\\n                            F.col(\"ORDER_AMOUNT\"), \\\n                            F.col(\"ORDER_TAX_AMOUNT\"), \\\n                            F.col(\"ORDER_DISCOUNT_AMOUNT\"), \\\n                            F.col(\"ORDER_TOTAL\"))\n    final_df.create_or_replace_view('POS_FLATTENED_V')\n\ndef create_pos_view_stream(session):\n    session.use_schema('HARMONIZED')\n    _ = session.sql('CREATE OR REPLACE STREAM POS_FLATTENED_V_STREAM \\\n                        ON VIEW POS_FLATTENED_V \\\n                        SHOW_INITIAL_ROWS = TRUE').collect()\n\ndef test_pos_view(session):\n    session.use_schema('HARMONIZED')\n    tv = session.table('POS_FLATTENED_V')\n    tv.limit(5).show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14352d-26ee-40a6-b1f5-0d94ffaa302a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "create_pos_view(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9076f-0620-4476-a94f-8d2b5897a2d6",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "create_pos_view_stream(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419aa2e8-87c8-446f-8236-c8dd2e2440c3",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "### Snowpark DataFrame API\nYou'll notice in the `create_pos_view()` function that we define the Snowflake view using the Snowpark DataFrame API. After defining the final DataFrame, which captures all the logic we want in the view, we can simply call the Snowpark `create_or_replace_view()` method. \n\nFor more details about the Snowpark Python DataFrame API, please check out our [Working with DataFrames in Snowpark Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes.html) page.\n\n### Streams for Incremental Processing (CDC)\nSnowflake makes processing data incrementally very easy. Traditionally the data engineer had to keep track of a high watermark (usually a datetime column) in order to process only new records in a table. This involved tracking and persisting that watermark somewhere and then using it in any query against the source table. But with Snowflake streams all the heavy lifting is done for you by Snowflake. For more details please check out our [Change Tracking Using Table Streams](https://docs.snowflake.com/en/user-guide/streams.html) user guide.\n\nAll you need to do is create a [`STREAM`](https://docs.snowflake.com/en/sql-reference/sql/create-stream.html) object in Snowflake against your base table or view, then query that stream just like any table in Snowflake. The stream will return only the changed records since the last DML option your performed. To help you work with the changed records, Snowflake streams will supply the following metadata columns along with the base table or view columns:\n\n* METADATA$ACTION\n* METADATA$ISUPDATE\n* METADATA$ROW_ID\n\nFor more details about these stream metadata columns please check out the [Stream Columns](https://docs.snowflake.com/en/user-guide/streams-intro.html#stream-columns) section in our documentation.\n\n### Streams on views\nWhat's really cool about Snowflake's incremental/CDC stream capability is the ability to create a stream on a view! In this example we are creating a stream on a view which joins together 6 of the raw POS tables. Here is the code to do that:\n\n```python\ndef create_pos_view_stream(session):\n    session.use_schema('HARMONIZED')\n    _ = session.sql('CREATE OR REPLACE STREAM POS_FLATTENED_V_STREAM \\\n                        ON VIEW POS_FLATTENED_V \\\n                        SHOW_INITIAL_ROWS = TRUE').collect()\n```\n\nNow when we query the `POS_FLATTENED_V_STREAM` stream to find changed records, Snowflake is actually looking for changed records in any of the 6 tables included in the view. For those who have tried to build incremental/CDC processes around denormalized schemas like this, you will appreciate the incredibly powerful feature that Snowflake provides here.\n\nFor more details please check out the [Streams on Views](https://docs.snowflake.com/en/user-guide/streams-intro.html#streams-on-views) section in our documentation.\n"
  },
  {
   "cell_type": "markdown",
   "id": "33f62ff3-5016-4514-8571-ed6195353e07",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell22"
   },
   "source": "## Fahrenheit to Celsius UDF\n\nDuring this step we will be creating and deploying a Snowpark Python user-defined function (UDF). The UDF provides a reusable calculation to simplify and standardize across users and pipelines, providing consistency and reducing development effort.\n\nTo put this in context, we are on step **#5** in our data flow overview."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f247e1-40d0-4003-92d5-3a19712254f1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION NB_HOL_DB.ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF(\"temp_f\" FLOAT)\n",
    "RETURNS FLOAT\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION=3.8\n",
    "HANDLER = 'main'\n",
    "AS '\n",
    "def main(temp_f: float) -> float:\n",
    "    return (float(temp_f) - 32) * (5/9)\n",
    "';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e72e42-4bf6-454c-b76c-e323fcfdbca8",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "### Running the UDF in Snowflake\nIn order to run the UDF in Snowflake you have a few options. Any UDF in Snowflake can be invoked inline through SQL as follows, and later we'll use it as part of our pipeline:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070c5e8a-f1e7-4fde-9b8e-e3514e3856e6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "SELECT NB_HOL_DB.ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF(35);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123bc413-3c14-4092-bc61-d6cc021704e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell26"
   },
   "source": "## Orders Update Stored Procedure\n\nDuring this step we will be creating and deploying our first Snowpark Python stored procedure (or sproc) to Snowflake. This sproc will merge changes from the `HARMONIZED.POS_FLATTENED_V_STREAM` stream into our target `HARMONIZED.ORDERS` table. To put this in context, we are on step **#6** in our data flow overview."
  },
  {
   "cell_type": "markdown",
   "id": "e0c8a50b-4dbb-4222-afd7-2fe8690dba54",
   "metadata": {
    "name": "cell27a",
    "collapsed": false
   },
   "source": "### TESTING the Sproc Locally\nFirst, to test the procedure locally, we will write the literal code inline, and then call the python method directly here in our notebook.\n\nSo go ahead and define the python methods:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75b295-bd82-4ed6-a565-259a56053c61",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell27b"
   },
   "outputs": [],
   "source": "\nimport time\nfrom snowflake.snowpark import Session\n#import snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n\ndef table_exists(session, schema='', name=''):\n    exists = session.sql(\"SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(schema, name)).collect()[0]['TABLE_EXISTS']\n    return exists\n\ndef create_orders_table(session):\n    _ = session.sql(\"CREATE TABLE HARMONIZED.ORDERS LIKE HARMONIZED.POS_FLATTENED_V\").collect()\n    _ = session.sql(\"ALTER TABLE HARMONIZED.ORDERS ADD COLUMN META_UPDATED_AT TIMESTAMP\").collect()\n\ndef create_orders_stream(session):\n    _ = session.sql(\"CREATE STREAM HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS\").collect()\n\ndef merge_order_updates(session):\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE').collect()\n\n    source = session.table('HARMONIZED.POS_FLATTENED_V_STREAM')\n    target = session.table('HARMONIZED.ORDERS')\n    print(\"{} records in stream\".format(session.table('HARMONIZED.POS_FLATTENED_V_STREAM').count()))\n\n    # TODO: Is the if clause supposed to be based on \"META_UPDATED_AT\"?\n    cols_to_update = {c: source[c] for c in source.schema.names if \"METADATA\" not in c}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    updates = {**cols_to_update, **metadata_col_to_update}\n\n    # merge into DIM_CUSTOMER\n    target.merge(source, target['ORDER_DETAIL_ID'] == source['ORDER_DETAIL_ID'], \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL').collect()\n\ndef main(session: Session) -> str:\n    # Create the ORDERS table and ORDERS_STREAM stream if they don't exist\n    if not table_exists(session, schema='HARMONIZED', name='ORDERS'):\n        create_orders_table(session)\n        create_orders_stream(session)\n\n    # Process data incrementally\n    merge_order_updates(session)\n#    session.table('HARMONIZED.ORDERS').limit(5).show()\n\n    return f\"Successfully processed ORDERS\""
  },
  {
   "cell_type": "markdown",
   "id": "408c76d5-dbdd-4ded-b5b1-509dfcdf1ef6",
   "metadata": {
    "name": "cell28a",
    "collapsed": false
   },
   "source": "And then execute your recently-defined `main()` method"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faffa0-45a3-48e2-aa01-db9c512b0881",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell28b"
   },
   "outputs": [],
   "source": "session.use_database('NB_HOL_DB')\nmain(session)"
  },
  {
   "cell_type": "markdown",
   "id": "3563ea82-f0e0-4497-92ef-1aec1e4b7534",
   "metadata": {
    "name": "cell28c",
    "collapsed": false
   },
   "source": "Take a look at the data we just loaded during our test.\n\nNote the META_UPDATED_AT time -- that was entered dynamically, so should be just a few seconds ago!"
  },
  {
   "cell_type": "code",
   "id": "c939d0a5-ee89-4a95-844f-e7c96a5035d2",
   "metadata": {
    "language": "sql",
    "name": "cell28d",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT ORDER_ID,ORDER_TS,META_UPDATED_AT,TRUCK_BRAND_NAME,PRIMARY_CITY,MENU_ITEM_NAME,QUANTITY,UNIT_PRICE \nFROM HARMONIZED.ORDERS \nLIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e73b081b-ee3c-4021-a396-1076c03ca4f3",
   "metadata": {
    "name": "cell29a",
    "collapsed": false
   },
   "source": "Now that we know our script works, we'll pop it into a giant string called 'script', and then use that string when we deploy the sproc in the subsequent cell:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bb06b-273a-4bba-8155-a4180ec90245",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell29b"
   },
   "outputs": [],
   "source": "script = '''\nimport time\nfrom snowflake.snowpark import Session\n#import snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n\ndef table_exists(session, schema='', name=''):\n    exists = session.sql(\"SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(schema, name)).collect()[0]['TABLE_EXISTS']\n    return exists\n\ndef create_orders_table(session):\n    _ = session.sql(\"CREATE TABLE HARMONIZED.ORDERS LIKE HARMONIZED.POS_FLATTENED_V\").collect()\n    _ = session.sql(\"ALTER TABLE HARMONIZED.ORDERS ADD COLUMN META_UPDATED_AT TIMESTAMP\").collect()\n\ndef create_orders_stream(session):\n    _ = session.sql(\"CREATE STREAM HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS\").collect()\n\ndef merge_order_updates(session):\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE').collect()\n\n    source = session.table('HARMONIZED.POS_FLATTENED_V_STREAM')\n    target = session.table('HARMONIZED.ORDERS')\n    print(\"{} records in stream\".format(session.table('HARMONIZED.POS_FLATTENED_V_STREAM').count()))\n\n    # TODO: Is the if clause supposed to be based on \"META_UPDATED_AT\"?\n    cols_to_update = {c: source[c] for c in source.schema.names if \"METADATA\" not in c}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    updates = {**cols_to_update, **metadata_col_to_update}\n\n    # merge into DIM_CUSTOMER\n    target.merge(source, target['ORDER_DETAIL_ID'] == source['ORDER_DETAIL_ID'], \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL').collect()\n\ndef main(session: Session) -> str:\n    # Create the ORDERS table and ORDERS_STREAM stream if they don't exist\n    if not table_exists(session, schema='HARMONIZED', name='ORDERS'):\n        create_orders_table(session)\n        create_orders_stream(session)\n\n    # Process data incrementally\n    merge_order_updates(session)\n#    session.table('HARMONIZED.ORDERS').limit(5).show()\n\n    return f\"Successfully processed ORDERS\"\n'''"
  },
  {
   "cell_type": "markdown",
   "id": "4de69c48-7dd4-4e15-bb2b-3cd46f8a5972",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": [
    "Here is the SQL query to deploy the procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c8215-aa83-4041-b0d0-61918a81b250",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell31"
   },
   "outputs": [],
   "source": "USE NB_HOL_DB.HARMONIZED;\nCREATE OR REPLACE PROCEDURE orders_update_sp()\n RETURNS string\n LANGUAGE PYTHON\n RUNTIME_VERSION=3.8\n PACKAGES=('snowflake-snowpark-python','toml')\n HANDLER = 'main'\n AS $$\n {{script}}\n $$;"
  },
  {
   "cell_type": "markdown",
   "id": "2e3d5d2d-6afc-413b-a7d9-d2dd068ab1d9",
   "metadata": {
    "name": "cell32a",
    "collapsed": false
   },
   "source": "Now, instead of directly calling the `main()` function, we'll use SQL to `CALL` the deployed SProc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7aacb-152a-4b53-b83a-a24578b27333",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "cell32b"
   },
   "outputs": [],
   "source": "USE NB_HOL_DB.HARMONIZED;\nCALL ORDERS_UPDATE_SP();"
  },
  {
   "cell_type": "markdown",
   "id": "46f5f6a9-334c-41be-b69f-72b0b2ff51bd",
   "metadata": {
    "name": "cell33",
    "collapsed": false
   },
   "source": "### More on the Snowpark API\nIn this step we're starting to really use the Snowpark DataFrame API for data transformations. To begin you need to create a Snowpark session object. When running in a Snowflake Notebook, the session object is provisioned for you automatically by Snowflake. And when building a Snowpark Python sproc the contract is that the first argument to the entry point (or handler) function is a Snowpark session.\n\nYou'll notice in the script that we have some functions which use SQL to create objects in Snowflake and to check object status. To issue a SQL statement to Snowflake with the Snowpark API you use the `session.sql()` function. Here's one example:\n\n```python\ndef create_orders_stream(session):\n    _ = session.sql(\"CREATE STREAM IF NOT EXISTS HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS \\\n                    SHOW_INITIAL_ROWS = TRUE;\").collect()\n```\n\nThe second thing to point out is how we're using DataFrames to merge changes from the source view to the target table. The Snowpark DataFrame API provides a `merge()` method which will ultimately generate a `MERGE` command in Snowflake.\n\nAgain, for more details about the Snowpark Python DataFrame API, please check out our [Working with DataFrames in Snowpark Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes.html) page.\n"
  },
  {
   "cell_type": "markdown",
   "id": "fd90d2f2-a045-4425-8ad0-7533c7d7611c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell34"
   },
   "source": [
    "## Daily City Metrics Update Sproc\n",
    "During this step we will be creating and deploying our second Snowpark Python sproc to Snowflake. This sproc will join the `HARMONIZED.ORDERS` data with the Weather Source data to create a final, aggregated table for analysis named `ANALYTICS.DAILY_CITY_METRICS`. We will process the data incrementally from the `HARMONIZED.ORDERS` table using another Snowflake Stream. And we will again use the Snowpark DataFrame `merge()` method to merge/upsert the data. To put this in context, we are on step **#7** in our data flow overview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05402dad-2e8c-413a-b8e7-20dc41c95312",
   "metadata": {
    "name": "cell35a",
    "collapsed": false
   },
   "source": "Once again, we'll start by writing the literal functions inline, for testing, and then call the `main()` function directly from here in the notebook.\nRun the direct function definitions:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fa22c-f646-4ab8-aea4-6ac17e5b96e3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell35b"
   },
   "outputs": [],
   "source": "import time\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n\ndef table_exists(session, schema='', name=''):\n    exists = session.sql(\"SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(schema, name)).collect()[0]['TABLE_EXISTS']\n    return exists\n\ndef create_daily_city_metrics_table(session):\n    SHARED_COLUMNS= [T.StructField(\"DATE\", T.DateType()),\n                                        T.StructField(\"CITY_NAME\", T.StringType()),\n                                        T.StructField(\"COUNTRY_DESC\", T.StringType()),\n                                        T.StructField(\"DAILY_SALES\", T.StringType()),\n                                        T.StructField(\"AVG_TEMPERATURE_FAHRENHEIT\", T.DecimalType()),\n                                        T.StructField(\"AVG_TEMPERATURE_CELSIUS\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_INCHES\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_MILLIMETERS\", T.DecimalType()),\n                                        T.StructField(\"MAX_WIND_SPEED_100M_MPH\", T.DecimalType()),\n                                    ]\n    DAILY_CITY_METRICS_COLUMNS = [*SHARED_COLUMNS, T.StructField(\"META_UPDATED_AT\", T.TimestampType())]\n    DAILY_CITY_METRICS_SCHEMA = T.StructType(DAILY_CITY_METRICS_COLUMNS)\n\n    dcm = session.create_dataframe([[None]*len(DAILY_CITY_METRICS_SCHEMA.names)], schema=DAILY_CITY_METRICS_SCHEMA) \\\n                        .na.drop() \\\n                        .write.mode('overwrite').save_as_table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n\n\ndef merge_daily_city_metrics(session):\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE').collect()\n\n    print(\"{} records in stream\".format(session.table('HARMONIZED.ORDERS_STREAM').count()))\n    orders_stream_dates = session.table('HARMONIZED.ORDERS_STREAM').select(F.col(\"ORDER_TS_DATE\").alias(\"DATE\")).distinct()\n    \n\n    orders = session.table(\"HARMONIZED.ORDERS_STREAM\").group_by(F.col('ORDER_TS_DATE'), F.col('PRIMARY_CITY'), F.col('COUNTRY')) \\\n                                        .agg(F.sum(F.col(\"PRICE\")).as_(\"price_nulls\")) \\\n                                        .with_column(\"DAILY_SALES\", F.call_builtin(\"ZEROIFNULL\", F.col(\"price_nulls\"))) \\\n                                        .select(F.col('ORDER_TS_DATE').alias(\"DATE\"), F.col(\"PRIMARY_CITY\").alias(\"CITY_NAME\"), \\\n                                        F.col(\"COUNTRY\").alias(\"COUNTRY_DESC\"), F.col(\"DAILY_SALES\"))\n#    orders.limit(5).show()\n\n    weather_pc = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES\")\n    countries = session.table(\"RAW_POS.COUNTRY\")\n    weather = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.HISTORY_DAY\")\n    weather = weather.join(weather_pc, (weather['POSTAL_CODE'] == weather_pc['POSTAL_CODE']) & (weather['COUNTRY'] == weather_pc['COUNTRY']), rsuffix='_pc')\n    weather = weather.join(countries, (weather['COUNTRY'] == countries['ISO_COUNTRY']) & (weather['CITY_NAME'] == countries['CITY']), rsuffix='_c')\n    weather = weather.join(orders_stream_dates, weather['DATE_VALID_STD'] == orders_stream_dates['DATE'])\n\n    weather_agg = weather.group_by(F.col('DATE_VALID_STD'), F.col('CITY_NAME'), F.col('COUNTRY_C')) \\\n                        .agg( \\\n                            F.avg('AVG_TEMPERATURE_AIR_2M_F').alias(\"AVG_TEMPERATURE_F\"), \\\n                            F.avg(F.call_udf(\"ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF\", F.col(\"AVG_TEMPERATURE_AIR_2M_F\"))).alias(\"AVG_TEMPERATURE_C\"), \\\n                            F.avg(\"TOT_PRECIPITATION_IN\").alias(\"AVG_PRECIPITATION_IN\"), \\\n                            F.avg(F.call_udf(\"ANALYTICS.INCH_TO_MILLIMETER_UDF\", F.col(\"TOT_PRECIPITATION_IN\"))).alias(\"AVG_PRECIPITATION_MM\"), \\\n                            F.max(F.col(\"MAX_WIND_SPEED_100M_MPH\")).alias(\"MAX_WIND_SPEED_100M_MPH\") \\\n                        ) \\\n                        .select(F.col(\"DATE_VALID_STD\").alias(\"DATE\"), F.col(\"CITY_NAME\"), F.col(\"COUNTRY_C\").alias(\"COUNTRY_DESC\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_F\"), 2).alias(\"AVG_TEMPERATURE_FAHRENHEIT\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_C\"), 2).alias(\"AVG_TEMPERATURE_CELSIUS\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_IN\"), 2).alias(\"AVG_PRECIPITATION_INCHES\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_MM\"), 2).alias(\"AVG_PRECIPITATION_MILLIMETERS\"), \\\n                            F.col(\"MAX_WIND_SPEED_100M_MPH\")\n                            )\n#    weather_agg.limit(5).show()\n\n    daily_city_metrics_stg = orders.join(weather_agg, (orders['DATE'] == weather_agg['DATE']) & (orders['CITY_NAME'] == weather_agg['CITY_NAME']) & (orders['COUNTRY_DESC'] == weather_agg['COUNTRY_DESC']), \\\n                        how='left', rsuffix='_w') \\\n                    .select(\"DATE\", \"CITY_NAME\", \"COUNTRY_DESC\", \"DAILY_SALES\", \\\n                        \"AVG_TEMPERATURE_FAHRENHEIT\", \"AVG_TEMPERATURE_CELSIUS\", \\\n                        \"AVG_PRECIPITATION_INCHES\", \"AVG_PRECIPITATION_MILLIMETERS\", \\\n                        \"MAX_WIND_SPEED_100M_MPH\")\n#    daily_city_metrics_stg.limit(5).show()\n\n    cols_to_update = {c: daily_city_metrics_stg[c] for c in daily_city_metrics_stg.schema.names}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    updates = {**cols_to_update, **metadata_col_to_update}\n\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm.merge(daily_city_metrics_stg, (dcm['DATE'] == daily_city_metrics_stg['DATE']) & (dcm['CITY_NAME'] == daily_city_metrics_stg['CITY_NAME']) & (dcm['COUNTRY_DESC'] == daily_city_metrics_stg['COUNTRY_DESC']), \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL').collect()\n\ndef main(session: Session) -> str:\n    # Create the DAILY_CITY_METRICS table if it doesn't exist\n    if not table_exists(session, schema='ANALYTICS', name='DAILY_CITY_METRICS'):\n        create_daily_city_metrics_table(session)\n    \n    merge_daily_city_metrics(session)\n#    session.table('ANALYTICS.DAILY_CITY_METRICS').limit(5).show()\n\n    return f\"Successfully processed DAILY_CITY_METRICS\""
  },
  {
   "cell_type": "markdown",
   "id": "0a83bb94-399d-49be-abef-10bfdc4a1872",
   "metadata": {
    "name": "cell36a",
    "collapsed": false
   },
   "source": "Now let's run the `main()` function to test our code before plunking it into a string variable for deployment:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddce8d6-8d33-4ec5-ba24-6ce352f3ee0e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell36b"
   },
   "outputs": [],
   "source": [
    "main(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadb325-1f5a-4ce5-baef-8643bb22dd03",
   "metadata": {
    "collapsed": false,
    "name": "cell37"
   },
   "source": "### Deploying the Sproc to Snowflake\nThe test code looks good, so let's copy it into a string:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c908df-3f1c-49ae-97e5-b89e159bd44b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "script = '''\nimport time\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n\ndef table_exists(session, schema='', name=''):\n    exists = session.sql(\"SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(schema, name)).collect()[0]['TABLE_EXISTS']\n    return exists\n\ndef create_daily_city_metrics_table(session):\n    SHARED_COLUMNS= [T.StructField(\"DATE\", T.DateType()),\n                                        T.StructField(\"CITY_NAME\", T.StringType()),\n                                        T.StructField(\"COUNTRY_DESC\", T.StringType()),\n                                        T.StructField(\"DAILY_SALES\", T.StringType()),\n                                        T.StructField(\"AVG_TEMPERATURE_FAHRENHEIT\", T.DecimalType()),\n                                        T.StructField(\"AVG_TEMPERATURE_CELSIUS\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_INCHES\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_MILLIMETERS\", T.DecimalType()),\n                                        T.StructField(\"MAX_WIND_SPEED_100M_MPH\", T.DecimalType()),\n                                    ]\n    DAILY_CITY_METRICS_COLUMNS = [*SHARED_COLUMNS, T.StructField(\"META_UPDATED_AT\", T.TimestampType())]\n    DAILY_CITY_METRICS_SCHEMA = T.StructType(DAILY_CITY_METRICS_COLUMNS)\n\n    dcm = session.create_dataframe([[None]*len(DAILY_CITY_METRICS_SCHEMA.names)], schema=DAILY_CITY_METRICS_SCHEMA) \\\n                        .na.drop() \\\n                        .write.mode('overwrite').save_as_table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n\n\ndef merge_daily_city_metrics(session):\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE').collect()\n\n    print(\"{} records in stream\".format(session.table('HARMONIZED.ORDERS_STREAM').count()))\n    orders_stream_dates = session.table('HARMONIZED.ORDERS_STREAM').select(F.col(\"ORDER_TS_DATE\").alias(\"DATE\")).distinct()\n#   orders_stream_dates.limit(5).show()\n\n    orders = session.table(\"HARMONIZED.ORDERS_STREAM\").group_by(F.col('ORDER_TS_DATE'), F.col('PRIMARY_CITY'), F.col('COUNTRY')) \\\n                                        .agg(F.sum(F.col(\"PRICE\")).as_(\"price_nulls\")) \\\n                                        .with_column(\"DAILY_SALES\", F.call_builtin(\"ZEROIFNULL\", F.col(\"price_nulls\"))) \\\n                                        .select(F.col('ORDER_TS_DATE').alias(\"DATE\"), F.col(\"PRIMARY_CITY\").alias(\"CITY_NAME\"), \\\n                                        F.col(\"COUNTRY\").alias(\"COUNTRY_DESC\"), F.col(\"DAILY_SALES\"))\n#    orders.limit(5).show()\n\n    weather_pc = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES\")\n    countries = session.table(\"RAW_POS.COUNTRY\")\n    weather = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.HISTORY_DAY\")\n    weather = weather.join(weather_pc, (weather['POSTAL_CODE'] == weather_pc['POSTAL_CODE']) & (weather['COUNTRY'] == weather_pc['COUNTRY']), rsuffix='_pc')\n    weather = weather.join(countries, (weather['COUNTRY'] == countries['ISO_COUNTRY']) & (weather['CITY_NAME'] == countries['CITY']), rsuffix='_c')\n    weather = weather.join(orders_stream_dates, weather['DATE_VALID_STD'] == orders_stream_dates['DATE'])\n\n    weather_agg = weather.group_by(F.col('DATE_VALID_STD'), F.col('CITY_NAME'), F.col('COUNTRY_C')) \\\n                        .agg( \\\n                            F.avg('AVG_TEMPERATURE_AIR_2M_F').alias(\"AVG_TEMPERATURE_F\"), \\\n                            F.avg(F.call_udf(\"ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF\", F.col(\"AVG_TEMPERATURE_AIR_2M_F\"))).alias(\"AVG_TEMPERATURE_C\"), \\\n                            F.avg(\"TOT_PRECIPITATION_IN\").alias(\"AVG_PRECIPITATION_IN\"), \\\n                            F.avg(F.call_udf(\"ANALYTICS.INCH_TO_MILLIMETER_UDF\", F.col(\"TOT_PRECIPITATION_IN\"))).alias(\"AVG_PRECIPITATION_MM\"), \\\n                            F.max(F.col(\"MAX_WIND_SPEED_100M_MPH\")).alias(\"MAX_WIND_SPEED_100M_MPH\") \\\n                        ) \\\n                        .select(F.col(\"DATE_VALID_STD\").alias(\"DATE\"), F.col(\"CITY_NAME\"), F.col(\"COUNTRY_C\").alias(\"COUNTRY_DESC\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_F\"), 2).alias(\"AVG_TEMPERATURE_FAHRENHEIT\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_C\"), 2).alias(\"AVG_TEMPERATURE_CELSIUS\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_IN\"), 2).alias(\"AVG_PRECIPITATION_INCHES\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_MM\"), 2).alias(\"AVG_PRECIPITATION_MILLIMETERS\"), \\\n                            F.col(\"MAX_WIND_SPEED_100M_MPH\")\n                            )\n#    weather_agg.limit(5).show()\n\n    daily_city_metrics_stg = orders.join(weather_agg, (orders['DATE'] == weather_agg['DATE']) & (orders['CITY_NAME'] == weather_agg['CITY_NAME']) & (orders['COUNTRY_DESC'] == weather_agg['COUNTRY_DESC']), \\\n                        how='left', rsuffix='_w') \\\n                    .select(\"DATE\", \"CITY_NAME\", \"COUNTRY_DESC\", \"DAILY_SALES\", \\\n                        \"AVG_TEMPERATURE_FAHRENHEIT\", \"AVG_TEMPERATURE_CELSIUS\", \\\n                        \"AVG_PRECIPITATION_INCHES\", \"AVG_PRECIPITATION_MILLIMETERS\", \\\n                        \"MAX_WIND_SPEED_100M_MPH\")\n#    daily_city_metrics_stg.limit(5).show()\n\n    cols_to_update = {c: daily_city_metrics_stg[c] for c in daily_city_metrics_stg.schema.names}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    updates = {**cols_to_update, **metadata_col_to_update}\n\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm.merge(daily_city_metrics_stg, (dcm['DATE'] == daily_city_metrics_stg['DATE']) & (dcm['CITY_NAME'] == daily_city_metrics_stg['CITY_NAME']) & (dcm['COUNTRY_DESC'] == daily_city_metrics_stg['COUNTRY_DESC']), \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL').collect()\n\ndef main(session: Session) -> str:\n    # Create the DAILY_CITY_METRICS table if it doesn't exist\n    if not table_exists(session, schema='ANALYTICS', name='DAILY_CITY_METRICS'):\n        create_daily_city_metrics_table(session)\n    \n    merge_daily_city_metrics(session)\n#    session.table('ANALYTICS.DAILY_CITY_METRICS').limit(5).show()\n\n    return f\"Successfully processed DAILY_CITY_METRICS\"\n'''"
  },
  {
   "cell_type": "markdown",
   "id": "69e9f854-71d7-4f3b-aae7-61eb7ec76443",
   "metadata": {
    "name": "cell39a",
    "collapsed": false
   },
   "source": "Then deploy our sproc:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b166f6-97d8-4d08-8c41-f49c0aae1aec",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell39b"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE PROCEDURE DAILY_CITY_METRICS_UPDATE_SP()\n",
    " RETURNS string\n",
    " LANGUAGE PYTHON\n",
    " RUNTIME_VERSION=3.8\n",
    " PACKAGES=('snowflake-snowpark-python','toml')\n",
    " HANDLER = 'main'\n",
    " AS $$\n",
    " {{script}}\n",
    " $$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9446717-c53a-41f2-9a80-6ebc5548ad9c",
   "metadata": {
    "name": "cell40",
    "collapsed": false
   },
   "source": "### Running the Sproc in Snowflake\nAnd finally, let's run it! To put this in context, we are on step **#7** in our data flow overview.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5289e-5197-4bda-ac42-551c609787b5",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "cell41"
   },
   "outputs": [],
   "source": [
    "CALL DAILY_CITY_METRICS_UPDATE_SP();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9f51b-0ba8-40ee-9524-a52086cafc56",
   "metadata": {
    "name": "cell42",
    "collapsed": false
   },
   "source": "### Data Modeling Best Practice\nWhen modeling data for analysis a best practice has been to clearly define and manage the schema of the table. In step 2, when we loaded raw data from Parquet we took advantage of Snowflake's schema detection feature to create a table with the same schema as the Parquet files. In this step we are explicitly defining the schema in DataFrame syntax and using that to create the table.\n\n### Complex Aggregation Query\nThe `merge_daily_city_metrics()` function contains a complex aggregation query which is used to join together and aggregate the data from our POS and Weather Source. Take a look at the series of complex series of joins and aggregations that are expressed, and how we're even leveraging the Snowpark UDF we created in step #5!\n\nThe complex aggregation query is then merged into the final analytics table using the Snowpark `merge()` method. If you haven't already, check out your Snowflake Query history and see which queries were generated by the Snowpark API. In this case you will see that the Snowpark API took all the complex logic, including the merge and created a single Snowflake query to execute!\n"
  },
  {
   "cell_type": "markdown",
   "id": "cffe9540-9d13-4b5b-94c7-07acbd52b970",
   "metadata": {
    "name": "cell43",
    "collapsed": false
   },
   "source": "## That's it for part 1 of the Lab!\nProceed to part 2 of the lab in a separate notebook"
  },
  {
   "cell_type": "markdown",
   "id": "995813d5-85d5-4f1a-8ab0-2667a7ed1d4d",
   "metadata": {
    "name": "cell44",
    "collapsed": false
   },
   "source": ""
  }
 ]
}